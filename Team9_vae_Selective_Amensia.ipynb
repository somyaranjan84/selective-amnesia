{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jjSweMgPHCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23f2d668-efb8-48bf-f7d2-7998d2b037aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/\n",
        "!git clone https://github.com/clear-nus/selective-amnesia"
      ],
      "metadata": {
        "id": "IyeVQhSmPLH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f0375e5-9f8c-44af-86aa-55e7b9ebae5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "fatal: destination path 'selective-amnesia' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q condacolab\n",
        "    import condacolab\n",
        "    condacolab.install()\n",
        "\n",
        "# Create a new environment (replace 'my_colab_env' with your desired name)\n",
        "environment_name = \"sa-vae\"\n",
        "!conda create --name sa-vae python=3.8 -y\n",
        "\n",
        "# Activate the environment and check Python version and installed packages\n",
        "!source activate sa-vae && python --version && pip list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1huld26Puxi",
        "outputId": "b3619bf5-b83b-4d3a-8843-91b16a8386a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:13\n",
            "🔁 Restarting kernel...\n",
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 24.11.2\n",
            "    latest version: 25.3.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/sa-vae\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.8\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2025.4.26  |       hbd8a1cb_0         149 KB  conda-forge\n",
            "    ld_impl_linux-64-2.43      |       h712a8e2_4         656 KB  conda-forge\n",
            "    libffi-3.4.6               |       h2dba641_1          56 KB  conda-forge\n",
            "    libgcc-15.1.0              |       h767d61c_2         810 KB  conda-forge\n",
            "    libgcc-ng-15.1.0           |       h69a702a_2          34 KB  conda-forge\n",
            "    libgomp-15.1.0             |       h767d61c_2         442 KB  conda-forge\n",
            "    liblzma-5.8.1              |       hb9d3cd8_1         110 KB  conda-forge\n",
            "    liblzma-devel-5.8.1        |       hb9d3cd8_1         431 KB  conda-forge\n",
            "    libsqlite-3.49.2           |       hee588c1_0         895 KB  conda-forge\n",
            "    ncurses-6.5                |       h2d0b736_3         871 KB  conda-forge\n",
            "    openssl-3.5.0              |       h7b32b05_1         3.0 MB  conda-forge\n",
            "    pip-24.3.1                 |     pyh8b19718_0         1.2 MB  conda-forge\n",
            "    python-3.8.20              |h4a871b0_2_cpython        21.1 MB  conda-forge\n",
            "    readline-8.2               |       h8c095d6_2         276 KB  conda-forge\n",
            "    setuptools-75.3.0          |     pyhd8ed1ab_0         761 KB  conda-forge\n",
            "    wheel-0.45.1               |     pyhd8ed1ab_0          62 KB  conda-forge\n",
            "    xz-5.8.1                   |       hbcc6ac9_1          23 KB  conda-forge\n",
            "    xz-gpl-tools-5.8.1         |       hbcc6ac9_1          33 KB  conda-forge\n",
            "    xz-tools-5.8.1             |       hb9d3cd8_1          94 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        30.9 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n",
            "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h4bc722e_7 \n",
            "  ca-certificates    conda-forge/noarch::ca-certificates-2025.4.26-hbd8a1cb_0 \n",
            "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.43-h712a8e2_4 \n",
            "  libffi             conda-forge/linux-64::libffi-3.4.6-h2dba641_1 \n",
            "  libgcc             conda-forge/linux-64::libgcc-15.1.0-h767d61c_2 \n",
            "  libgcc-ng          conda-forge/linux-64::libgcc-ng-15.1.0-h69a702a_2 \n",
            "  libgomp            conda-forge/linux-64::libgomp-15.1.0-h767d61c_2 \n",
            "  liblzma            conda-forge/linux-64::liblzma-5.8.1-hb9d3cd8_1 \n",
            "  liblzma-devel      conda-forge/linux-64::liblzma-devel-5.8.1-hb9d3cd8_1 \n",
            "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hd590300_0 \n",
            "  libsqlite          conda-forge/linux-64::libsqlite-3.49.2-hee588c1_0 \n",
            "  libuuid            conda-forge/linux-64::libuuid-2.38.1-h0b41bf4_0 \n",
            "  libxcrypt          conda-forge/linux-64::libxcrypt-4.4.36-hd590300_1 \n",
            "  libzlib            conda-forge/linux-64::libzlib-1.3.1-hb9d3cd8_2 \n",
            "  ncurses            conda-forge/linux-64::ncurses-6.5-h2d0b736_3 \n",
            "  openssl            conda-forge/linux-64::openssl-3.5.0-h7b32b05_1 \n",
            "  pip                conda-forge/noarch::pip-24.3.1-pyh8b19718_0 \n",
            "  python             conda-forge/linux-64::python-3.8.20-h4a871b0_2_cpython \n",
            "  readline           conda-forge/linux-64::readline-8.2-h8c095d6_2 \n",
            "  setuptools         conda-forge/noarch::setuptools-75.3.0-pyhd8ed1ab_0 \n",
            "  tk                 conda-forge/linux-64::tk-8.6.13-noxft_h4845f30_101 \n",
            "  wheel              conda-forge/noarch::wheel-0.45.1-pyhd8ed1ab_0 \n",
            "  xz                 conda-forge/linux-64::xz-5.8.1-hbcc6ac9_1 \n",
            "  xz-gpl-tools       conda-forge/linux-64::xz-gpl-tools-5.8.1-hbcc6ac9_1 \n",
            "  xz-tools           conda-forge/linux-64::xz-tools-5.8.1-hb9d3cd8_1 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "python-3.8.20        | 21.1 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "openssl-3.5.0        | 3.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "pip-24.3.1           | 1.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libsqlite-3.49.2     | 895 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.1.0        | 810 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-75.3.0    | 761 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 656 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.1.0       | 442 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-devel-5.8.1  | 431 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 149 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-tools-5.8.1       | 94 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wheel-0.45.1         | 62 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libffi-3.4.6         | 56 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-15.1.0     | 34 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-gpl-tools-5.8.1   | 33 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.8.20        | 21.1 MB   | :   0% 0.001477632678048695/1 [00:00<01:11, 71.81s/it]\n",
            "openssl-3.5.0        | 3.0 MB    | :   1% 0.010511289820716556/1 [00:00<00:10, 10.39s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | :   4% 0.03675021673521069/1 [00:00<00:02,  2.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libsqlite-3.49.2     | 895 KB    | :   2% 0.017880353110782014/1 [00:00<00:06,  6.23s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pip-24.3.1           | 1.2 MB    | :   1% 0.013179232412674715/1 [00:00<00:09,  9.53s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | : 100% 1.0/1 [00:00<00:00,  2.88s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "openssl-3.5.0        | 3.0 MB    | :  90% 0.9039709245816239/1 [00:00<00:00,  5.10it/s]  \u001b[A\n",
            "\n",
            "\n",
            "python-3.8.20        | 21.1 MB   | :  10% 0.10195665478535997/1 [00:00<00:01,  1.83s/it] \n",
            "\n",
            "pip-24.3.1           | 1.2 MB    | : 100% 1.0/1 [00:00<00:00,  9.53s/it]                 \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.1.0        | 810 KB    | :   2% 0.019760996154903825/1 [00:00<00:10, 10.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-75.3.0    | 761 KB    | :   2% 0.02101695697963341/1 [00:00<00:11, 11.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.1.0        | 810 KB    | : 100% 1.0/1 [00:00<00:00, 10.76s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 656 KB    | :   2% 0.02440855729694297/1 [00:00<00:10, 11.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-75.3.0    | 761 KB    | : 100% 1.0/1 [00:00<00:00, 11.38s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.8.20        | 21.1 MB   | :  18% 0.17657710502681906/1 [00:00<00:01,  1.58s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.1.0       | 442 KB    | :   4% 0.03619693572083467/1 [00:00<00:08,  8.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "openssl-3.5.0        | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  5.10it/s]               \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-devel-5.8.1  | 431 KB    | :   4% 0.03710212141524303/1 [00:00<00:08,  8.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.1.0       | 442 KB    | : 100% 1.0/1 [00:00<00:00,  8.38s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | :   6% 0.05800056641178136/1 [00:00<00:05,  5.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-devel-5.8.1  | 431 KB    | : 100% 1.0/1 [00:00<00:00,  8.82s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | : 100% 1.0/1 [00:00<00:00,  5.96s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 149 KB    | :  11% 0.10758915965669182/1 [00:00<00:03,  3.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 149 KB    | : 100% 1.0/1 [00:00<00:00,  3.37s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | :  15% 0.1451903052860118/1 [00:00<00:02,  2.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-tools-5.8.1       | 94 KB     | :  17% 0.1704287764994695/1 [00:00<00:01,  2.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | : 100% 1.0/1 [00:00<00:00,  2.64s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.8.20        | 21.1 MB   | :  30% 0.2970041682877877/1 [00:00<00:00,  1.17s/it] \n",
            "\n",
            "\n",
            "libsqlite-3.49.2     | 895 KB    | : 100% 1.0/1 [00:00<00:00,  2.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libsqlite-3.49.2     | 895 KB    | : 100% 1.0/1 [00:00<00:00,  2.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wheel-0.45.1         | 62 KB     | :  26% 0.26007174830947016/1 [00:00<00:01,  1.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wheel-0.45.1         | 62 KB     | : 100% 1.0/1 [00:00<00:00,  1.60s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libffi-3.4.6         | 56 KB     | :  29% 0.2852715337871955/1 [00:00<00:01,  1.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libffi-3.4.6         | 56 KB     | : 100% 1.0/1 [00:00<00:00,  1.46s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-gpl-tools-5.8.1   | 33 KB     | :  48% 0.4839891291504195/1 [00:00<00:00,  1.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-gpl-tools-5.8.1   | 33 KB     | : 100% 1.0/1 [00:00<00:00,  1.11it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-15.1.0     | 34 KB     | :  47% 0.47371768923842017/1 [00:00<00:00,  1.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-15.1.0     | 34 KB     | : 100% 1.0/1 [00:00<00:00,  1.05it/s]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-5.8.1             | 23 KB     | :  68% 0.6845777796348138/1 [00:00<00:00,  1.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.8.20        | 21.1 MB   | : 100% 1.0/1 [00:01<00:00,  1.04s/it]\n",
            "\n",
            "pip-24.3.1           | 1.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.10s/it]\u001b[A\u001b[A\n",
            "\n",
            "pip-24.3.1           | 1.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.10s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.1.0        | 810 KB    | : 100% 1.0/1 [00:01<00:00,  1.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.1.0        | 810 KB    | : 100% 1.0/1 [00:01<00:00,  1.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-75.3.0    | 761 KB    | : 100% 1.0/1 [00:01<00:00,  1.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-75.3.0    | 761 KB    | : 100% 1.0/1 [00:01<00:00,  1.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 656 KB    | : 100% 1.0/1 [00:01<00:00,  1.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 656 KB    | : 100% 1.0/1 [00:01<00:00,  1.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "openssl-3.5.0        | 3.0 MB    | : 100% 1.0/1 [00:02<00:00,  5.10it/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.1.0       | 442 KB    | : 100% 1.0/1 [00:02<00:00,  2.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.1.0       | 442 KB    | : 100% 1.0/1 [00:02<00:00,  2.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-devel-5.8.1  | 431 KB    | : 100% 1.0/1 [00:02<00:00,  2.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-devel-5.8.1  | 431 KB    | : 100% 1.0/1 [00:02<00:00,  2.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | : 100% 1.0/1 [00:02<00:00,  2.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | : 100% 1.0/1 [00:02<00:00,  2.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 149 KB    | : 100% 1.0/1 [00:02<00:00,  2.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 149 KB    | : 100% 1.0/1 [00:02<00:00,  2.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | : 100% 1.0/1 [00:02<00:00,  2.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | : 100% 1.0/1 [00:02<00:00,  2.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | : 100% 1.0/1 [00:02<00:00,  2.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | : 100% 1.0/1 [00:02<00:00,  2.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-tools-5.8.1       | 94 KB     | : 100% 1.0/1 [00:02<00:00,  2.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-tools-5.8.1       | 94 KB     | : 100% 1.0/1 [00:02<00:00,  2.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wheel-0.45.1         | 62 KB     | : 100% 1.0/1 [00:02<00:00,  2.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wheel-0.45.1         | 62 KB     | : 100% 1.0/1 [00:02<00:00,  2.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libffi-3.4.6         | 56 KB     | : 100% 1.0/1 [00:02<00:00,  2.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libffi-3.4.6         | 56 KB     | : 100% 1.0/1 [00:02<00:00,  2.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-gpl-tools-5.8.1   | 33 KB     | : 100% 1.0/1 [00:02<00:00,  2.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-gpl-tools-5.8.1   | 33 KB     | : 100% 1.0/1 [00:02<00:00,  2.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-5.8.1             | 23 KB     | : 100% 1.0/1 [00:02<00:00,  3.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-5.8.1             | 23 KB     | : 100% 1.0/1 [00:02<00:00,  3.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-15.1.0     | 34 KB     | : 100% 1.0/1 [00:02<00:00,  2.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate sa-vae\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "Python 3.8.20\n",
            "Package    Version\n",
            "---------- -------\n",
            "pip        24.3.1\n",
            "setuptools 75.3.0\n",
            "wheel      0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nzIMvVNcRB8K",
        "outputId": "5596be0f-a035-4887-e54b-e63ca938571d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting certifi==2023.5.7 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 1))\n",
            "  Downloading certifi-2023.5.7-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting charset-normalizer==3.1.0 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 2))\n",
            "  Downloading charset_normalizer-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\n",
            "Collecting cmake==3.26.3 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 3))\n",
            "  Downloading cmake-3.26.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting filelock==3.12.0 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 4))\n",
            "  Downloading filelock-3.12.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting idna==3.4 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 5))\n",
            "  Downloading idna-3.4-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting Jinja2==3.1.2 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 6))\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting lit==16.0.5 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 7))\n",
            "  Downloading lit-16.0.5.tar.gz (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting MarkupSafe==2.1.2 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 8))\n",
            "  Downloading MarkupSafe-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 9)) (1.3.0)\n",
            "Collecting networkx==3.1 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 10))\n",
            "  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting numpy==1.24.3 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 11))\n",
            "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 12))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 13))\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 14))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 15))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 16))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 17))\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 18))\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 19))\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 20))\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 21))\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 22))\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting Pillow==9.5.0 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 23))\n",
            "  Downloading Pillow-9.5.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting PyYAML==6.0 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 24))\n",
            "  Downloading PyYAML-6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting requests==2.30.0 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 25))\n",
            "  Downloading requests-2.30.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting sympy==1.12 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 26))\n",
            "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting torch==2.0.1 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 27))\n",
            "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchvision==0.15.2 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 28))\n",
            "  Downloading torchvision-0.15.2-cp311-cp311-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Collecting tqdm==4.65.0 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 29))\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 30))\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting typing_extensions==4.5.0 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 31))\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting urllib3==2.0.2 (from -r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 32))\n",
            "  Downloading urllib3-2.0.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->-r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 12)) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->-r /content/drive/MyDrive/selective-amnesia/vae/requirements.txt (line 12)) (0.45.1)\n",
            "Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.0/157.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (197 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.3/197.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmake-3.26.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
            "Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27 kB)\n",
            "Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Pillow-9.5.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.9/757.9 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.30.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.15.2-cp311-cp311-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Downloading urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.2/123.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: lit\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-16.0.5-py3-none-any.whl size=88176 sha256=33e1a848840ab5aedd053ccbaeb19325780bebab444951d6b11d64b5b8c3d489\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/eb/f9/d234956aef3a2b49ece71268ce26b1f545fc965c51c827f14a\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, cmake, urllib3, typing_extensions, tqdm, sympy, PyYAML, Pillow, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, networkx, MarkupSafe, idna, filelock, charset-normalizer, certifi, requests, nvidia-cusolver-cu11, nvidia-cudnn-cu11, Jinja2, triton, torch, torchvision\n",
            "  Attempting uninstall: cmake\n",
            "    Found existing installation: cmake 3.31.6\n",
            "    Uninstalling cmake-3.31.6:\n",
            "      Successfully uninstalled cmake-3.31.6\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 11.2.1\n",
            "    Uninstalling pillow-11.2.1:\n",
            "      Successfully uninstalled pillow-11.2.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.2\n",
            "    Uninstalling charset-normalizer-3.4.2:\n",
            "      Successfully uninstalled charset-normalizer-3.4.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.4.26\n",
            "    Uninstalling certifi-2025.4.26:\n",
            "      Successfully uninstalled certifi-2025.4.26\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.30.0 which is incompatible.\n",
            "pydantic 2.11.4 requires typing-extensions>=4.12.2, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "curl-cffi 0.10.0 requires certifi>=2024.2.2, but you have certifi 2023.5.7 which is incompatible.\n",
            "sqlalchemy 2.0.40 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
            "nx-cugraph-cu12 25.2.0 requires networkx>=3.2, but you have networkx 3.1 which is incompatible.\n",
            "nibabel 5.3.2 requires typing-extensions>=4.6; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "dataproc-spark-connect 0.7.3 requires tqdm>=4.67, but you have tqdm 4.65.0 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "altair 5.5.0 requires typing-extensions>=4.10.0; python_version < \"3.14\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pytensor 2.30.3 requires filelock>=3.15, but you have filelock 3.12.0 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "scikit-image 0.25.2 requires pillow>=10.1, but you have pillow 9.5.0 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "pydantic-core 2.33.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "google-genai 1.15.0 requires typing-extensions<5.0.0,>=4.11.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "typing-inspection 0.4.0 requires typing-extensions>=4.12.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "yfinance 0.2.61 requires requests>=2.31, but you have requests 2.30.0 which is incompatible.\n",
            "typeguard 4.4.2 requires typing_extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "openai 1.78.1 requires typing-extensions<5,>=4.11, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "langchain-core 0.3.59 requires typing-extensions>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Jinja2-3.1.2 MarkupSafe-2.1.2 Pillow-9.5.0 PyYAML-6.0 certifi-2023.5.7 charset-normalizer-3.1.0 cmake-3.26.3 filelock-3.12.0 idna-3.4 lit-16.0.5 networkx-3.1 numpy-1.24.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 requests-2.30.0 sympy-1.12 torch-2.0.1 torchvision-0.15.2 tqdm-4.65.0 triton-2.0.0 typing_extensions-4.5.0 urllib3-2.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi",
                  "numpy"
                ]
              },
              "id": "bed988deae3841eeb307e3f18d544506"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "!python /content/drive/MyDrive/selective-amnesia/vae/train_cvae.py --config /content/drive/MyDrive/selective-amnesia/vae/mnist.yaml --data_path ./dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK_5dCCdVPNl",
        "outputId": "cfc32224-a814-4ace-dc4e-59936fa20309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO - train_cvae.py - 2025-05-23 22:44:14,455 - Beginning basic training of conditional VAE\n",
            "100% 9.91M/9.91M [00:01<00:00, 6.08MB/s]\n",
            "100% 28.9k/28.9k [00:00<00:00, 159kB/s]\n",
            "100% 1.65M/1.65M [00:01<00:00, 1.30MB/s]\n",
            "100% 4.54k/4.54k [00:00<00:00, 11.1MB/s]\n",
            "INFO - train_cvae.py - 2025-05-23 22:51:10,032 - Train Step: 4999 (5%)\t Avg Train Loss Per Batch: 36174.326254\t Avg Test Loss Per Batch: 28615.454242\n",
            "INFO - train_cvae.py - 2025-05-23 22:57:59,778 - Train Step: 9999 (10%)\t Avg Train Loss Per Batch: 28301.401032\t Avg Test Loss Per Batch: 26766.335791\n",
            "INFO - train_cvae.py - 2025-05-23 23:04:52,659 - Train Step: 14999 (15%)\t Avg Train Loss Per Batch: 26995.876732\t Avg Test Loss Per Batch: 26014.169101\n",
            "INFO - train_cvae.py - 2025-05-23 23:11:53,036 - Train Step: 19999 (20%)\t Avg Train Loss Per Batch: 26304.096200\t Avg Test Loss Per Batch: 25558.599667\n",
            "INFO - train_cvae.py - 2025-05-23 23:19:04,027 - Train Step: 24999 (25%)\t Avg Train Loss Per Batch: 25846.222766\t Avg Test Loss Per Batch: 25274.760452\n",
            "INFO - train_cvae.py - 2025-05-23 23:26:53,459 - Train Step: 29999 (30%)\t Avg Train Loss Per Batch: 25510.586666\t Avg Test Loss Per Batch: 25026.469791\n",
            "INFO - train_cvae.py - 2025-05-23 23:34:59,053 - Train Step: 34999 (35%)\t Avg Train Loss Per Batch: 25249.982265\t Avg Test Loss Per Batch: 24879.760071\n",
            "INFO - train_cvae.py - 2025-05-23 23:43:17,397 - Train Step: 39999 (40%)\t Avg Train Loss Per Batch: 25031.983576\t Avg Test Loss Per Batch: 24726.198013\n",
            "INFO - train_cvae.py - 2025-05-23 23:51:48,218 - Train Step: 44999 (45%)\t Avg Train Loss Per Batch: 24857.874110\t Avg Test Loss Per Batch: 24618.202676\n",
            "INFO - train_cvae.py - 2025-05-24 00:00:27,585 - Train Step: 49999 (50%)\t Avg Train Loss Per Batch: 24710.342207\t Avg Test Loss Per Batch: 24563.917944\n",
            "INFO - train_cvae.py - 2025-05-24 00:09:10,109 - Train Step: 54999 (55%)\t Avg Train Loss Per Batch: 24578.170106\t Avg Test Loss Per Batch: 24468.393832\n",
            "INFO - train_cvae.py - 2025-05-24 00:17:48,326 - Train Step: 59999 (60%)\t Avg Train Loss Per Batch: 24469.225037\t Avg Test Loss Per Batch: 24418.655380\n",
            "INFO - train_cvae.py - 2025-05-24 00:26:27,646 - Train Step: 64999 (65%)\t Avg Train Loss Per Batch: 24371.591879\t Avg Test Loss Per Batch: 24361.167697\n",
            "INFO - train_cvae.py - 2025-05-24 00:35:08,291 - Train Step: 69999 (70%)\t Avg Train Loss Per Batch: 24281.682425\t Avg Test Loss Per Batch: 24329.444757\n",
            "INFO - train_cvae.py - 2025-05-24 00:43:50,797 - Train Step: 74999 (75%)\t Avg Train Loss Per Batch: 24198.755036\t Avg Test Loss Per Batch: 24302.430936\n",
            "INFO - train_cvae.py - 2025-05-24 00:52:35,309 - Train Step: 79999 (80%)\t Avg Train Loss Per Batch: 24131.374623\t Avg Test Loss Per Batch: 24250.969916\n",
            "INFO - train_cvae.py - 2025-05-24 01:01:21,791 - Train Step: 84999 (85%)\t Avg Train Loss Per Batch: 24064.621201\t Avg Test Loss Per Batch: 24230.889182\n",
            "INFO - train_cvae.py - 2025-05-24 01:10:00,489 - Train Step: 89999 (90%)\t Avg Train Loss Per Batch: 24003.851972\t Avg Test Loss Per Batch: 24208.143854\n",
            "INFO - train_cvae.py - 2025-05-24 01:18:39,951 - Train Step: 94999 (95%)\t Avg Train Loss Per Batch: 23945.688613\t Avg Test Loss Per Batch: 24209.360764\n",
            "INFO - train_cvae.py - 2025-05-24 01:27:17,566 - Train Step: 99999 (100%)\t Avg Train Loss Per Batch: 23896.941392\t Avg Test Loss Per Batch: 24177.731061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "!python /content/drive/MyDrive/selective-amnesia/vae/calculate_fim.py --ckpt_folder /content/results/mnist/2025_05_23_224414"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVikJnBVCdUR",
        "outputId": "08b6f819-bdbe-486f-97ea-eaa27f43aa84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 50000/50000 [17:28<00:00, 47.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "!python /content/drive/MyDrive/selective-amnesia/vae/train_forget.py --ckpt_folder /content/results/mnist/2025_05_23_224414 --label_to_drop 0 --lmbda 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI3feG_xIa--",
        "outputId": "10c4d058-60f0-49d1-ac5b-3635f5065a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO - train_forget.py - 2025-05-24 01:54:41,369 - CVAE forgetting training.\n",
            "INFO - train_forget.py - 2025-05-24 01:54:41,369 - Digit to drop: 0, lambda: 100.0, gamma: 1\n",
            "INFO - train_forget.py - 2025-05-24 01:55:12,303 - Train Step: 199 (2%)\t Avg Train Loss Per Batch: 653802.009531\n",
            "INFO - train_forget.py - 2025-05-24 01:55:12,304 - Avg Forgetting Loss Per Batch: 653716.687500\t Avg EWC Loss Per Batch\n",
            ": 85.170815\n",
            "INFO - train_forget.py - 2025-05-24 01:55:41,043 - Train Step: 399 (4%)\t Avg Train Loss Per Batch: 187333.634688\n",
            "INFO - train_forget.py - 2025-05-24 01:55:41,043 - Avg Forgetting Loss Per Batch: 187124.984375\t Avg EWC Loss Per Batch\n",
            ": 208.632278\n",
            "INFO - train_forget.py - 2025-05-24 01:56:10,629 - Train Step: 599 (6%)\t Avg Train Loss Per Batch: 172674.851797\n",
            "INFO - train_forget.py - 2025-05-24 01:56:10,629 - Avg Forgetting Loss Per Batch: 172424.046875\t Avg EWC Loss Per Batch\n",
            ": 250.763290\n",
            "INFO - train_forget.py - 2025-05-24 01:56:39,154 - Train Step: 799 (8%)\t Avg Train Loss Per Batch: 171512.856953\n",
            "INFO - train_forget.py - 2025-05-24 01:56:39,154 - Avg Forgetting Loss Per Batch: 171241.921875\t Avg EWC Loss Per Batch\n",
            ": 270.926941\n",
            "INFO - train_forget.py - 2025-05-24 01:57:07,794 - Train Step: 999 (10%)\t Avg Train Loss Per Batch: 171073.687187\n",
            "INFO - train_forget.py - 2025-05-24 01:57:07,794 - Avg Forgetting Loss Per Batch: 170786.390625\t Avg EWC Loss Per Batch\n",
            ": 287.317993\n",
            "INFO - train_forget.py - 2025-05-24 01:57:36,385 - Train Step: 1199 (12%)\t Avg Train Loss Per Batch: 170624.314922\n",
            "INFO - train_forget.py - 2025-05-24 01:57:36,385 - Avg Forgetting Loss Per Batch: 170319.500000\t Avg EWC Loss Per Batch\n",
            ": 304.864227\n",
            "INFO - train_forget.py - 2025-05-24 01:58:05,106 - Train Step: 1399 (14%)\t Avg Train Loss Per Batch: 170192.314922\n",
            "INFO - train_forget.py - 2025-05-24 01:58:05,107 - Avg Forgetting Loss Per Batch: 169868.296875\t Avg EWC Loss Per Batch\n",
            ": 323.972321\n",
            "INFO - train_forget.py - 2025-05-24 01:58:33,639 - Train Step: 1599 (16%)\t Avg Train Loss Per Batch: 169868.768984\n",
            "INFO - train_forget.py - 2025-05-24 01:58:33,639 - Avg Forgetting Loss Per Batch: 169524.765625\t Avg EWC Loss Per Batch\n",
            ": 343.987915\n",
            "INFO - train_forget.py - 2025-05-24 01:59:02,094 - Train Step: 1799 (18%)\t Avg Train Loss Per Batch: 169683.088984\n",
            "INFO - train_forget.py - 2025-05-24 01:59:02,094 - Avg Forgetting Loss Per Batch: 169318.984375\t Avg EWC Loss Per Batch\n",
            ": 364.097839\n",
            "INFO - train_forget.py - 2025-05-24 01:59:30,681 - Train Step: 1999 (20%)\t Avg Train Loss Per Batch: 169374.150469\n",
            "INFO - train_forget.py - 2025-05-24 01:59:30,681 - Avg Forgetting Loss Per Batch: 168989.375000\t Avg EWC Loss Per Batch\n",
            ": 384.793823\n",
            "INFO - train_forget.py - 2025-05-24 01:59:59,259 - Train Step: 2199 (22%)\t Avg Train Loss Per Batch: 169213.376953\n",
            "INFO - train_forget.py - 2025-05-24 01:59:59,259 - Avg Forgetting Loss Per Batch: 168807.546875\t Avg EWC Loss Per Batch\n",
            ": 405.845337\n",
            "INFO - train_forget.py - 2025-05-24 02:00:27,819 - Train Step: 2399 (24%)\t Avg Train Loss Per Batch: 168996.712891\n",
            "INFO - train_forget.py - 2025-05-24 02:00:27,819 - Avg Forgetting Loss Per Batch: 168569.046875\t Avg EWC Loss Per Batch\n",
            ": 427.679688\n",
            "INFO - train_forget.py - 2025-05-24 02:00:56,376 - Train Step: 2599 (26%)\t Avg Train Loss Per Batch: 168871.930000\n",
            "INFO - train_forget.py - 2025-05-24 02:00:56,377 - Avg Forgetting Loss Per Batch: 168422.046875\t Avg EWC Loss Per Batch\n",
            ": 449.790741\n",
            "INFO - train_forget.py - 2025-05-24 02:01:25,138 - Train Step: 2799 (28%)\t Avg Train Loss Per Batch: 168759.344062\n",
            "INFO - train_forget.py - 2025-05-24 02:01:25,138 - Avg Forgetting Loss Per Batch: 168287.609375\t Avg EWC Loss Per Batch\n",
            ": 471.775940\n",
            "INFO - train_forget.py - 2025-05-24 02:01:57,888 - Train Step: 2999 (30%)\t Avg Train Loss Per Batch: 168583.684844\n",
            "INFO - train_forget.py - 2025-05-24 02:01:57,888 - Avg Forgetting Loss Per Batch: 168090.453125\t Avg EWC Loss Per Batch\n",
            ": 493.229065\n",
            "INFO - train_forget.py - 2025-05-24 02:02:26,689 - Train Step: 3199 (32%)\t Avg Train Loss Per Batch: 168525.781641\n",
            "INFO - train_forget.py - 2025-05-24 02:02:26,690 - Avg Forgetting Loss Per Batch: 168011.640625\t Avg EWC Loss Per Batch\n",
            ": 514.148621\n",
            "INFO - train_forget.py - 2025-05-24 02:02:55,422 - Train Step: 3399 (34%)\t Avg Train Loss Per Batch: 168363.279531\n",
            "INFO - train_forget.py - 2025-05-24 02:02:55,423 - Avg Forgetting Loss Per Batch: 167828.812500\t Avg EWC Loss Per Batch\n",
            ": 534.502441\n",
            "INFO - train_forget.py - 2025-05-24 02:03:24,134 - Train Step: 3599 (36%)\t Avg Train Loss Per Batch: 168272.956875\n",
            "INFO - train_forget.py - 2025-05-24 02:03:24,134 - Avg Forgetting Loss Per Batch: 167718.796875\t Avg EWC Loss Per Batch\n",
            ": 554.139587\n",
            "INFO - train_forget.py - 2025-05-24 02:03:52,787 - Train Step: 3799 (38%)\t Avg Train Loss Per Batch: 168072.406094\n",
            "INFO - train_forget.py - 2025-05-24 02:03:52,788 - Avg Forgetting Loss Per Batch: 167500.125000\t Avg EWC Loss Per Batch\n",
            ": 572.275391\n",
            "INFO - train_forget.py - 2025-05-24 02:04:21,350 - Train Step: 3999 (40%)\t Avg Train Loss Per Batch: 168045.114922\n",
            "INFO - train_forget.py - 2025-05-24 02:04:21,350 - Avg Forgetting Loss Per Batch: 167456.125000\t Avg EWC Loss Per Batch\n",
            ": 589.010132\n",
            "INFO - train_forget.py - 2025-05-24 02:04:50,242 - Train Step: 4199 (42%)\t Avg Train Loss Per Batch: 167879.385234\n",
            "INFO - train_forget.py - 2025-05-24 02:04:50,242 - Avg Forgetting Loss Per Batch: 167274.734375\t Avg EWC Loss Per Batch\n",
            ": 604.639526\n",
            "INFO - train_forget.py - 2025-05-24 02:05:19,274 - Train Step: 4399 (44%)\t Avg Train Loss Per Batch: 167808.471953\n",
            "INFO - train_forget.py - 2025-05-24 02:05:19,274 - Avg Forgetting Loss Per Batch: 167190.125000\t Avg EWC Loss Per Batch\n",
            ": 618.338196\n",
            "INFO - train_forget.py - 2025-05-24 02:05:48,093 - Train Step: 4599 (46%)\t Avg Train Loss Per Batch: 167674.369844\n",
            "INFO - train_forget.py - 2025-05-24 02:05:48,093 - Avg Forgetting Loss Per Batch: 167043.515625\t Avg EWC Loss Per Batch\n",
            ": 630.819214\n",
            "INFO - train_forget.py - 2025-05-24 02:06:16,931 - Train Step: 4799 (48%)\t Avg Train Loss Per Batch: 167594.291797\n",
            "INFO - train_forget.py - 2025-05-24 02:06:16,931 - Avg Forgetting Loss Per Batch: 166952.812500\t Avg EWC Loss Per Batch\n",
            ": 641.486267\n",
            "INFO - train_forget.py - 2025-05-24 02:06:45,685 - Train Step: 4999 (50%)\t Avg Train Loss Per Batch: 167506.207656\n",
            "INFO - train_forget.py - 2025-05-24 02:06:45,685 - Avg Forgetting Loss Per Batch: 166855.125000\t Avg EWC Loss Per Batch\n",
            ": 651.088196\n",
            "INFO - train_forget.py - 2025-05-24 02:07:14,663 - Train Step: 5199 (52%)\t Avg Train Loss Per Batch: 167396.168203\n",
            "INFO - train_forget.py - 2025-05-24 02:07:14,663 - Avg Forgetting Loss Per Batch: 166737.062500\t Avg EWC Loss Per Batch\n",
            ": 659.067017\n",
            "INFO - train_forget.py - 2025-05-24 02:07:43,551 - Train Step: 5399 (54%)\t Avg Train Loss Per Batch: 167360.362422\n",
            "INFO - train_forget.py - 2025-05-24 02:07:43,551 - Avg Forgetting Loss Per Batch: 166694.609375\t Avg EWC Loss Per Batch\n",
            ": 665.693298\n",
            "INFO - train_forget.py - 2025-05-24 02:08:12,639 - Train Step: 5599 (56%)\t Avg Train Loss Per Batch: 167257.561406\n",
            "INFO - train_forget.py - 2025-05-24 02:08:12,640 - Avg Forgetting Loss Per Batch: 166586.062500\t Avg EWC Loss Per Batch\n",
            ": 671.484802\n",
            "INFO - train_forget.py - 2025-05-24 02:08:41,661 - Train Step: 5799 (58%)\t Avg Train Loss Per Batch: 167119.616328\n",
            "INFO - train_forget.py - 2025-05-24 02:08:41,661 - Avg Forgetting Loss Per Batch: 166443.156250\t Avg EWC Loss Per Batch\n",
            ": 676.507019\n",
            "INFO - train_forget.py - 2025-05-24 02:09:10,271 - Train Step: 5999 (60%)\t Avg Train Loss Per Batch: 167124.519453\n",
            "INFO - train_forget.py - 2025-05-24 02:09:10,271 - Avg Forgetting Loss Per Batch: 166443.765625\t Avg EWC Loss Per Batch\n",
            ": 680.717163\n",
            "INFO - train_forget.py - 2025-05-24 02:09:39,039 - Train Step: 6199 (62%)\t Avg Train Loss Per Batch: 166989.809219\n",
            "INFO - train_forget.py - 2025-05-24 02:09:39,039 - Avg Forgetting Loss Per Batch: 166305.625000\t Avg EWC Loss Per Batch\n",
            ": 684.208069\n",
            "INFO - train_forget.py - 2025-05-24 02:10:07,551 - Train Step: 6399 (64%)\t Avg Train Loss Per Batch: 166988.531797\n",
            "INFO - train_forget.py - 2025-05-24 02:10:07,551 - Avg Forgetting Loss Per Batch: 166300.171875\t Avg EWC Loss Per Batch\n",
            ": 688.374084\n",
            "INFO - train_forget.py - 2025-05-24 02:10:35,924 - Train Step: 6599 (66%)\t Avg Train Loss Per Batch: 166920.057188\n",
            "INFO - train_forget.py - 2025-05-24 02:10:35,925 - Avg Forgetting Loss Per Batch: 166228.171875\t Avg EWC Loss Per Batch\n",
            ": 691.896362\n",
            "INFO - train_forget.py - 2025-05-24 02:11:04,264 - Train Step: 6799 (68%)\t Avg Train Loss Per Batch: 166901.939766\n",
            "INFO - train_forget.py - 2025-05-24 02:11:04,264 - Avg Forgetting Loss Per Batch: 166206.890625\t Avg EWC Loss Per Batch\n",
            ": 694.999390\n",
            "INFO - train_forget.py - 2025-05-24 02:11:32,634 - Train Step: 6999 (70%)\t Avg Train Loss Per Batch: 166830.684219\n",
            "INFO - train_forget.py - 2025-05-24 02:11:32,635 - Avg Forgetting Loss Per Batch: 166132.718750\t Avg EWC Loss Per Batch\n",
            ": 697.957153\n",
            "INFO - train_forget.py - 2025-05-24 02:12:04,699 - Train Step: 7199 (72%)\t Avg Train Loss Per Batch: 166849.161953\n",
            "INFO - train_forget.py - 2025-05-24 02:12:04,699 - Avg Forgetting Loss Per Batch: 166148.140625\t Avg EWC Loss Per Batch\n",
            ": 701.065857\n",
            "INFO - train_forget.py - 2025-05-24 02:12:33,029 - Train Step: 7399 (74%)\t Avg Train Loss Per Batch: 166754.346953\n",
            "INFO - train_forget.py - 2025-05-24 02:12:33,029 - Avg Forgetting Loss Per Batch: 166050.296875\t Avg EWC Loss Per Batch\n",
            ": 703.982910\n",
            "INFO - train_forget.py - 2025-05-24 02:13:01,166 - Train Step: 7599 (76%)\t Avg Train Loss Per Batch: 166766.000625\n",
            "INFO - train_forget.py - 2025-05-24 02:13:01,167 - Avg Forgetting Loss Per Batch: 166059.890625\t Avg EWC Loss Per Batch\n",
            ": 706.135010\n",
            "INFO - train_forget.py - 2025-05-24 02:13:29,483 - Train Step: 7799 (78%)\t Avg Train Loss Per Batch: 166665.238281\n",
            "INFO - train_forget.py - 2025-05-24 02:13:29,484 - Avg Forgetting Loss Per Batch: 165957.218750\t Avg EWC Loss Per Batch\n",
            ": 707.984863\n",
            "INFO - train_forget.py - 2025-05-24 02:13:57,816 - Train Step: 7999 (80%)\t Avg Train Loss Per Batch: 166636.113438\n",
            "INFO - train_forget.py - 2025-05-24 02:13:57,816 - Avg Forgetting Loss Per Batch: 165927.281250\t Avg EWC Loss Per Batch\n",
            ": 708.816772\n",
            "INFO - train_forget.py - 2025-05-24 02:14:26,163 - Train Step: 8199 (82%)\t Avg Train Loss Per Batch: 166634.196641\n",
            "INFO - train_forget.py - 2025-05-24 02:14:26,163 - Avg Forgetting Loss Per Batch: 165924.875000\t Avg EWC Loss Per Batch\n",
            ": 709.336548\n",
            "INFO - train_forget.py - 2025-05-24 02:14:54,435 - Train Step: 8399 (84%)\t Avg Train Loss Per Batch: 166596.455859\n",
            "INFO - train_forget.py - 2025-05-24 02:14:54,436 - Avg Forgetting Loss Per Batch: 165886.984375\t Avg EWC Loss Per Batch\n",
            ": 709.418213\n",
            "INFO - train_forget.py - 2025-05-24 02:15:22,575 - Train Step: 8599 (86%)\t Avg Train Loss Per Batch: 166582.077969\n",
            "INFO - train_forget.py - 2025-05-24 02:15:22,576 - Avg Forgetting Loss Per Batch: 165873.125000\t Avg EWC Loss Per Batch\n",
            ": 708.995605\n",
            "INFO - train_forget.py - 2025-05-24 02:15:50,737 - Train Step: 8799 (88%)\t Avg Train Loss Per Batch: 166536.756250\n",
            "INFO - train_forget.py - 2025-05-24 02:15:50,738 - Avg Forgetting Loss Per Batch: 165828.875000\t Avg EWC Loss Per Batch\n",
            ": 707.934265\n",
            "INFO - train_forget.py - 2025-05-24 02:16:19,017 - Train Step: 8999 (90%)\t Avg Train Loss Per Batch: 166506.043516\n",
            "INFO - train_forget.py - 2025-05-24 02:16:19,017 - Avg Forgetting Loss Per Batch: 165799.859375\t Avg EWC Loss Per Batch\n",
            ": 706.118103\n",
            "INFO - train_forget.py - 2025-05-24 02:16:47,197 - Train Step: 9199 (92%)\t Avg Train Loss Per Batch: 166507.873750\n",
            "INFO - train_forget.py - 2025-05-24 02:16:47,197 - Avg Forgetting Loss Per Batch: 165803.781250\t Avg EWC Loss Per Batch\n",
            ": 704.066284\n",
            "INFO - train_forget.py - 2025-05-24 02:17:15,394 - Train Step: 9399 (94%)\t Avg Train Loss Per Batch: 166491.079062\n",
            "INFO - train_forget.py - 2025-05-24 02:17:15,395 - Avg Forgetting Loss Per Batch: 165789.406250\t Avg EWC Loss Per Batch\n",
            ": 701.609436\n",
            "INFO - train_forget.py - 2025-05-24 02:17:43,489 - Train Step: 9599 (96%)\t Avg Train Loss Per Batch: 166474.965234\n",
            "INFO - train_forget.py - 2025-05-24 02:17:43,489 - Avg Forgetting Loss Per Batch: 165776.265625\t Avg EWC Loss Per Batch\n",
            ": 698.667603\n",
            "INFO - train_forget.py - 2025-05-24 02:18:11,803 - Train Step: 9799 (98%)\t Avg Train Loss Per Batch: 166473.884375\n",
            "INFO - train_forget.py - 2025-05-24 02:18:11,804 - Avg Forgetting Loss Per Batch: 165778.218750\t Avg EWC Loss Per Batch\n",
            ": 695.652283\n",
            "INFO - train_forget.py - 2025-05-24 02:18:40,239 - Train Step: 9999 (100%)\t Avg Train Loss Per Batch: 166466.507969\n",
            "INFO - train_forget.py - 2025-05-24 02:18:40,239 - Avg Forgetting Loss Per Batch: 165774.359375\t Avg EWC Loss Per Batch\n",
            ": 692.173279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "!python /content/drive/MyDrive/selective-amnesia/vae/train_classifier.py --data_path ./dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JVhoRTKQvxK",
        "outputId": "db0168f7-8fc1-4892-b72e-d6264f845fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "/content/drive/MyDrive/selective-amnesia/vae/model.py:61: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.341571\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.059943\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.811393\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.492217\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.195732\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.171436\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.806846\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.945575\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.814273\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.873429\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "\n",
            "Test set: Avg. loss: 0.3626, Accuracy: 9342/10000 (93%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.727223\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.682444\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.704021\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.548510\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.563527\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.472699\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.542742\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.431126\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.520525\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.358932\n",
            "\n",
            "Test set: Avg. loss: 0.1866, Accuracy: 9545/10000 (95%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.474884\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.450917\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.483621\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.430016\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.461739\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.446388\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.392580\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.346713\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.354397\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.386366\n",
            "\n",
            "Test set: Avg. loss: 0.1359, Accuracy: 9625/10000 (96%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.582064\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.428503\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.296076\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.449508\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.342615\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.359300\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.277743\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.164264\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.385317\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.328237\n",
            "\n",
            "Test set: Avg. loss: 0.1112, Accuracy: 9660/10000 (97%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.235221\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.176877\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.247360\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.255099\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.212634\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.377878\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.143994\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.260211\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.234935\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.312779\n",
            "\n",
            "Test set: Avg. loss: 0.0951, Accuracy: 9706/10000 (97%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.210747\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.208819\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.244099\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.212641\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.221976\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.259738\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.201013\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.361187\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.240975\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.133082\n",
            "\n",
            "Test set: Avg. loss: 0.0933, Accuracy: 9705/10000 (97%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.249990\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.368966\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.269446\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.187542\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.203444\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.272042\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.280894\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.284577\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.239924\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.383215\n",
            "\n",
            "Test set: Avg. loss: 0.0920, Accuracy: 9715/10000 (97%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.115919\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.141585\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.448561\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.291010\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.254321\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.404293\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.190670\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.170068\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.359258\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.135246\n",
            "\n",
            "Test set: Avg. loss: 0.0903, Accuracy: 9718/10000 (97%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.348982\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.421516\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.289085\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.296399\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.424341\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.263480\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.271927\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.107160\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.164653\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.386469\n",
            "\n",
            "Test set: Avg. loss: 0.0886, Accuracy: 9725/10000 (97%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.133601\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.290512\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.260383\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.336256\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.247321\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.161793\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.162197\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.139051\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.346483\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.489407\n",
            "\n",
            "Test set: Avg. loss: 0.0883, Accuracy: 9724/10000 (97%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.380978\n",
            "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.345116\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.317676\n",
            "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.322688\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.233878\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.261560\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.251957\n",
            "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.159974\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.235151\n",
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.130308\n",
            "\n",
            "Test set: Avg. loss: 0.0884, Accuracy: 9723/10000 (97%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.128828\n",
            "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.149891\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.238609\n",
            "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.309108\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.413652\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.153117\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.235490\n",
            "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.222887\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.291209\n",
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.183966\n",
            "\n",
            "Test set: Avg. loss: 0.0879, Accuracy: 9724/10000 (97%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.101909\n",
            "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.467864\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.231431\n",
            "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.307134\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.188002\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.287488\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.507912\n",
            "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.257034\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.289317\n",
            "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.230017\n",
            "\n",
            "Test set: Avg. loss: 0.0881, Accuracy: 9724/10000 (97%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.234670\n",
            "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.308881\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.220570\n",
            "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.285986\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.244895\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.156207\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.219127\n",
            "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.224566\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.260142\n",
            "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.085954\n",
            "\n",
            "Test set: Avg. loss: 0.0877, Accuracy: 9723/10000 (97%)\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.456861\n",
            "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.215905\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.354263\n",
            "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.266039\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.220121\n",
            "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.176932\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.374853\n",
            "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.287487\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.321772\n",
            "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.184265\n",
            "\n",
            "Test set: Avg. loss: 0.0878, Accuracy: 9723/10000 (97%)\n",
            "\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.121957\n",
            "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 0.296296\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.303390\n",
            "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.192171\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.135787\n",
            "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.164863\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.413856\n",
            "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 0.212537\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.252227\n",
            "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.305080\n",
            "\n",
            "Test set: Avg. loss: 0.0879, Accuracy: 9723/10000 (97%)\n",
            "\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.212452\n",
            "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 0.296743\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.217847\n",
            "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.202007\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.108882\n",
            "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.354767\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.181987\n",
            "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 0.156740\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.268578\n",
            "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.252575\n",
            "\n",
            "Test set: Avg. loss: 0.0879, Accuracy: 9724/10000 (97%)\n",
            "\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.147444\n",
            "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 0.095556\n",
            "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.126137\n",
            "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.126864\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.291220\n",
            "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.192242\n",
            "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.292921\n",
            "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 0.180511\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.224465\n",
            "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.104507\n",
            "\n",
            "Test set: Avg. loss: 0.0876, Accuracy: 9724/10000 (97%)\n",
            "\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.227349\n",
            "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 0.392043\n",
            "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.177604\n",
            "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 0.379734\n",
            "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.430430\n",
            "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.232462\n",
            "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.287835\n",
            "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 0.247508\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.274208\n",
            "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.330845\n",
            "\n",
            "Test set: Avg. loss: 0.0874, Accuracy: 9722/10000 (97%)\n",
            "\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.338159\n",
            "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 0.355122\n",
            "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.146364\n",
            "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 0.141182\n",
            "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.184246\n",
            "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 0.261541\n",
            "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.308393\n",
            "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 0.247947\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.310798\n",
            "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 0.346907\n",
            "\n",
            "Test set: Avg. loss: 0.0875, Accuracy: 9725/10000 (97%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "!python /content/drive/MyDrive/selective-amnesia/vae/generate_samples.py --ckpt_folder /content/results/mnist/2025_05_24_015441 --label_to_generate 0 --n_samples 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQwtLkAhWMR0",
        "outputId": "cc592f93-9755-4a85-aa1e-1d46fc709f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 1/1 [00:00<00:00,  1.51it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "!python /content/drive/MyDrive/selective-amnesia/vae/evaluate_with_classifier.py --sample_path /content/results/mnist/2025_05_24_015441 --label_of_dropped_class 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1y7X03gWk_6",
        "outputId": "d06a9e77-f4d5-4058-abf1-2003c5ea82e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average entropy: 0\n",
            "Average prob of forgotten class: 0\n"
          ]
        }
      ]
    }
  ]
}